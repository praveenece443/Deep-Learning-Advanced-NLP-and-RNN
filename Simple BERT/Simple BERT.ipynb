{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "print(\"TF version: \", tf.__version__)\n",
    "print(\"Hub version: \", hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For cleaning the text\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "import string\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# For building our model\n",
    "import tensorflow.keras\n",
    "import sklearn\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/mitramir55/Kaggle_NLP_competition.git\n",
    "train = pd.read_csv('Kaggle_NLP_competition/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Kaggle_NLP_competition/test.csv')\n",
    "sample_sub = pd.read_csv('Kaggle_NLP_competition/sample_submission.csv')\n",
    "test_id = test.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([train, test], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test, df_concat]:\n",
    "    df.keyword.fillna('no_keyword', inplace = True)\n",
    "    df.location.fillna('no_location', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 30 locations in the dataset\n",
    "top_30 = df_concat.groupby(['location']).count().text.sort_values(ascending = False)[:30]\n",
    "\n",
    "# plot the top 30\n",
    "\n",
    "plt.figure(figsize = (6,10))\n",
    "sns.barplot(x = top_30, y = top_30.index);\n",
    "plt.xlabel('number of tweets');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 20 keywords in disastrous and non_disastrous tweets\n",
    "# We'll use training set for this \n",
    "\n",
    "count_dis_keywords = train[train.target == 1].groupby(['keyword']).count().sort_values(by = 'target', ascending = False)[:20]\n",
    "count_non_dis_keywords =  train[train.target == 0].groupby(['keyword']).count().sort_values(by = 'target', ascending = False)[:20]\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "fig, ax_ = plt.subplots(1, 2, figsize = (25,10));\n",
    "\n",
    "# left side, the plot for keywords in disastrous tweets\n",
    "\n",
    "sns.barplot(x = count_dis_keywords.target, # count of each keyword\n",
    "            y = count_dis_keywords.index, # index of this df is our keywords\n",
    "            ax = ax_[0],\n",
    "            palette = 'Reds_r', label = 'dis')\n",
    "\n",
    "# right side, the plot for non_disastrous tweets\n",
    "\n",
    "sns.barplot(x = count_non_dis_keywords.target, y = count_non_dis_keywords.index, \n",
    "            ax = ax_[1], palette = 'Greens_d', label = 'non_dis')\n",
    "\n",
    "\n",
    "for ax in [ax_[0], ax_[1]]:\n",
    "    \n",
    "    ax.set_title('Number of tweets per keyword', fontsize = 20) # setting title\n",
    "    \n",
    "    ax.set_ylabel('') \n",
    "    ax.set_xlabel('')\n",
    "\n",
    "    ax.set_yticklabels(labels =ax.get_yticklabels() ,\n",
    "                       fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test, df_concat]:\n",
    "    df.drop(columns = ['location', 'keyword', 'id'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en\")\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# spacy (362 words)\n",
    "spacy_st = sp.Defaults.stop_words\n",
    "# nltk(179 words)\n",
    "nltk_st = stopwords.words('english')\n",
    "\n",
    "def clean(tweet, http = True, punc = True, lem = True, stop_w = True):\n",
    "    \n",
    "    if http is True:\n",
    "        tweet = re.sub(\"https?:\\/\\/t.co\\/[A-Za-z0-9]*\", '', tweet)\n",
    "\n",
    "    # stop words\n",
    "    # in here I changed the placement of lower for those of you who want to use\n",
    "    # Cased BERT later on.\n",
    "    if stop_w == 'nltk':\n",
    "        tweet = [word for word in word_tokenize(tweet) if not word.lower() in nltk_st]\n",
    "        tweet = ' '.join(tweet)\n",
    "\n",
    "    elif stop_w == 'spacy':\n",
    "        tweet = [word for word in word_tokenize(tweet) if not word.lower() in spacy_st]\n",
    "        tweet = ' '.join(tweet)\n",
    "\n",
    "    # lemmitizing\n",
    "    if lem == True:\n",
    "        lemmatized = [word.lemma_ for word in sp(tweet)]\n",
    "        tweet = ' '.join(lemmatized)\n",
    "\n",
    "    # punctuation removal\n",
    "    if punc is True:\n",
    "        tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "    # removing extra space\n",
    "    tweet = re.sub(\"\\s+\", ' ', tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['cleaned_text'] = df_concat.text.apply(lambda x: clean(x, lem = False, stop_w = 'nltk', http = True, punc = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train = df_concat[:train.shape[0]]\n",
    "cleaned_test = df_concat[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenization\n",
    "FullTokenizer = tokenization.FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = input(\"Which Bert should I use? \\n a. Base uncased \\n b. Large uncased \\n c. Basic cased \\n d. Large cased \\n\")\n",
    "\n",
    "if ans is 'a':\n",
    "    BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "    disc = 'Base_uncased'\n",
    "elif ans is 'b':\n",
    "    BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2' \n",
    "    disc = 'Large_uncased'\n",
    "elif ans is 'c':\n",
    "    BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2'\n",
    "    disc = 'Base_cased'\n",
    "elif ans is 'd':\n",
    "    BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/2'\n",
    "    disc = 'Large_cased'\n",
    "\n",
    "bert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)\n",
    "print('Bert layer is ready to use!')\n",
    "\n",
    "\n",
    "\n",
    "if ans =='a' or ans =='b':\n",
    "    to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "\n",
    "    tokenizer = FullTokenizer(vocabulary_file, to_lower_case)\n",
    "    \n",
    "\n",
    "    \n",
    "elif ans =='c' or ans =='d':\n",
    "    \n",
    "    vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    \n",
    "    tokenizer = FullTokenizer(vocabulary_file, do_lower_case=False)\n",
    "\n",
    "\n",
    "print('Bert Tokenizer is ready!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your own sentence here, try words like openminded, undercover, etc., and see what you get\n",
    "sentence = 'openminded'\n",
    "print(\"Tokenized version of '{}' is : \\n {}\".format(sentence, tokenizer.tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweets(text_):\n",
    "    return tokenizer.convert_tokens_to_ids(['[CLS]'] + tokenizer.tokenize(text_) + ['[SEP]'])\n",
    "\n",
    "\n",
    "df_concat['tokenized_tweets'] = df_concat.cleaned_text.apply(lambda x: tokenize_tweets(x))\n",
    "df_concat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = len(max(df_concat.tokenized_tweets, key = len))\n",
    "\n",
    "\n",
    "print('The maximum length of each sequence besed on tokenized tweets is:', max_len)\n",
    "\n",
    "df_concat['padded_tweets'] = df_concat.tokenized_tweets.apply(lambda x: x + [0] * (max_len - len(x)))\n",
    "df_concat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetClassifier:\n",
    "    \n",
    "    def __init__(self, tokenizer, bert_layer, max_len, lr = 0.0001,\n",
    "                 epochs = 15, batch_size = 32,\n",
    "                 activation = 'sigmoid', optimizer = 'Adam',\n",
    "                 beta_1=0.9, beta_2=0.999, epsilon=1e-07,\n",
    "                 metrics = 'accuracy', loss = 'binary_crossentropy'):\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert_layer = bert_layer     \n",
    "\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon =epsilon\n",
    "        \n",
    "        self.metrics = metrics\n",
    "        self.loss = loss\n",
    "\n",
    "        \n",
    "    def encode(self, texts):\n",
    "        \n",
    "        all_tokens = []\n",
    "        masks = []\n",
    "        segments = []\n",
    "        \n",
    "        for text in texts:\n",
    "            \n",
    "            tokenized = self.tokenizer.convert_tokens_to_ids(['[CLS]'] + self.tokenizer.tokenize(text) + ['[SEP]'])            \n",
    "            len_zeros = self.max_len - len(tokenized)          \n",
    "            \n",
    "            padded = tokenized + [0] * len_zeros\n",
    "            mask = [1] * len(tokenized) + [0] * len_zeros\n",
    "            segment = [0] * self.max_len\n",
    "            \n",
    "            all_tokens.append(padded)\n",
    "            masks.append(mask)\n",
    "            segments.append(segment)\n",
    "        return np.array(all_tokens), np.array(masks), np.array(segments)\n",
    "\n",
    "\n",
    "    def make_model(self):\n",
    "        \n",
    "        # Shaping the inputs to our model\n",
    "        \n",
    "        input_ids = Input(shape = (self.max_len, ), dtype = tf.int32, name = 'input_ids')        \n",
    "        input_mask = Input(shape = (self.max_len, ), dtype = tf.int32, name = 'input_mask')        \n",
    "        segment_ids = Input(shape = (self.max_len, ), dtype = tf.int32,  name = 'segment_ids')        \n",
    "        pooled_output, sequence_output = bert_layer([input_ids, input_mask, segment_ids] )\n",
    "\n",
    "\n",
    "\n",
    "        clf_output = sequence_output[:, 0, :]\n",
    "        \n",
    "        out = tf.keras.layers.Dense(1, activation = self.activation)(clf_output)\n",
    "        \n",
    "        \n",
    "        model = Model(inputs = [input_ids, input_mask, segment_ids], outputs = out)\n",
    "        \n",
    "        # define the optimizer\n",
    "\n",
    "        if self.optimizer is 'SGD':\n",
    "            optimizer = SGD(learning_rate = self.lr)\n",
    "\n",
    "        elif self.optimizer is 'Adam': \n",
    "            optimizer = Adam(learning_rate = self.lr, beta_1=self.beta_1, beta_2=self.beta_2, epsilon=self.epsilon)\n",
    "        model.compile(loss = self.loss, optimizer = self.optimizer, metrics = [self.metrics])        \n",
    "        print('Model is compiled with {} optimizer'.format(self.optimizer))        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, x):    \n",
    "        \n",
    "        checkpoint = ModelCheckpoint('model.h5', monitor='val_loss',\n",
    "                                     save_best_only=True)            \n",
    "        \n",
    "        model = self.make_model()\n",
    "        \n",
    "        X = self.encode(x['cleaned_text'])\n",
    "        Y = x['target']\n",
    "        \n",
    "        model.fit(X, Y, shuffle = True, validation_split = 0.2, \n",
    "                  batch_size=self.batch_size, epochs = self.epochs,\n",
    "                  callbacks=[checkpoint])\n",
    "                \n",
    "        print('Model is fit!')\n",
    "        \n",
    "            \n",
    "    def predict(self, x):\n",
    "        \n",
    "        X_test_encoded = self.encode(x['cleaned_text'])\n",
    "        best_model = tf.keras.models.load_model('model.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "        y_pred = best_model.predict(X_test_encoded)\n",
    "        \n",
    "        \n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TweetClassifier(tokenizer = tokenizer, bert_layer = bert_layer,\n",
    "                              max_len = max_len, lr = 0.0001,\n",
    "                              epochs = 10,  activation = 'sigmoid',\n",
    "                              batch_size = 32,optimizer = 'adam',\n",
    "                              beta_1=0.9, beta_2=0.999, epsilon=1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(cleaned_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore BERT's dictionary file\n",
    "with open(vocabulary_file,'r', encoding='utf-8') as f:\n",
    "    dic = f.readlines()\n",
    "pd.Series(dic)[pd.Series(dic).str.contains('cru')]\n",
    "#custom_objects={'KerasLayer':hub.KerasLayer}\n",
    "\n",
    "# Testing your predictions:\n",
    "\n",
    "perfection = pd.read_csv(r'Kaggle_NLP_competition/perfect_submission.csv')\n",
    "\n",
    "y_pred = np.round(classifier.predict(cleaned_test))\n",
    "print('The score of prediction: ', sklearn.metrics.f1_score(perfection.target, y_pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
